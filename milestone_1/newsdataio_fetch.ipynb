{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67e291-55cb-481f-91a8-26ce572d87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c4cfa5-f97b-485c-907a-27fa926da0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from your .env file\n",
    "load_dotenv()\n",
    "NEWSDATA_IO_KEY = os.getenv(\"NEWSDATA_IO_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248ca96-7cff-461e-963b-26449d9955d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"Artificial Intelligence\"\n",
    "TARGET_ARTICLE_COUNT = 200 \n",
    "REQUEST_DELAY_SECONDS = 5 \n",
    "\n",
    "# This list will hold all articles \n",
    "all_articles = []\n",
    "next_page_token = None # This token tells the API which page of results to get next\n",
    "\n",
    "print(f\"Starting paginated fetch from Newsdata.io. Target: {TARGET_ARTICLE_COUNT} articles.\")\n",
    "\n",
    "\n",
    "# loop for target number of news atricles\n",
    "while len(all_articles) < TARGET_ARTICLE_COUNT:\n",
    "    \n",
    "    \n",
    "    url = \"https://newsdata.io/api/1/news\"\n",
    "    params = {\n",
    "        \"q\": QUERY,\n",
    "        \"language\": \"en\",\n",
    "        \"apikey\": NEWSDATA_IO_KEY\n",
    "    }\n",
    "    \n",
    "    # If we have a token from a previous request, add it to the params to get the next page\n",
    "    if next_page_token:\n",
    "        params[\"page\"] = next_page_token\n",
    "\n",
    "    print(f\"Fetching page... (Collected {len(all_articles)} articles so far)\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}. Stopping.\")\n",
    "            break # Exit the loop on an error\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        if data.get(\"status\") != \"success\":\n",
    "            print(f\"API Error: {data.get('results', {}).get('message')}. Stopping.\")\n",
    "            break # Exit the loop on an error\n",
    "            \n",
    "        articles_on_this_page = data.get(\"results\", [])\n",
    "        \n",
    "        # This part ensures the columns match newsapi \n",
    "        for article in articles_on_this_page:\n",
    "            authors = \", \".join(article.get(\"creator\", [])) if article.get(\"creator\") else \"N/A\"\n",
    "            all_articles.append({\n",
    "                \"source\": article.get(\"source_id\", \"N/A\"),\n",
    "                \"author\": authors,\n",
    "                \"title\": article.get(\"title\"),\n",
    "                \"description\": article.get(\"description\"),\n",
    "                \"url\": article.get(\"link\"),\n",
    "                \"publishedAt\": article.get(\"pubDate\")\n",
    "            })\n",
    "\n",
    "        next_page_token = data.get(\"nextPage\")\n",
    "        if not next_page_token:\n",
    "            print(\"No more pages of results available.\")\n",
    "            break # Exit the loop if the API no more page available\n",
    "\n",
    "        # Wait before the next request to avoid limit finishing\n",
    "        time.sleep(REQUEST_DELAY_SECONDS)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"A network error occurred: {e}. Stopping.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f326d-8216-4a9e-ba8b-7ba8b859f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv\n",
    "if not all_articles:\n",
    "    print(\"\\nNo articles were collected.\")\n",
    "else:\n",
    "    # Convert the list into a DataFrame\n",
    "    df = pd.DataFrame(all_articles)\n",
    "\n",
    "    output_columns = [\n",
    "        \"source\", \"title\", \"description\", \"url\", \"publishedAt\"\n",
    "    ]\n",
    "    df = df[output_columns]\n",
    "\n",
    "    # Save to its own CSV file\n",
    "    df.to_csv(\"newsdata_io.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\nSuccess!\")\n",
    "    print(f\"Saved a total of {len(df)} articles to 'newsdata_io.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
